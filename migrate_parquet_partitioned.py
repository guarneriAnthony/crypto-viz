#!/usr/bin/env python3
"""
Script de migration des parquets vers structure partitionnÃ©e Y/M/D
Lit les anciens parquets de crypto-data et les rÃ©Ã©crit avec partitioning
"""

import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from datetime import datetime

def create_spark_session():
    """Session Spark pour migration"""
    return SparkSession.builder \
        .appName("CryptoViz-Migration-Partitioned") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "cryptoviz") \
        .config("spark.hadoop.fs.s3a.secret.key", "cryptoviz2024") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.jars.packages", 
               "org.apache.hadoop:hadoop-aws:3.3.4,"
               "com.amazonaws:aws-java-sdk-bundle:1.12.262") \
        .getOrCreate()

def migrate_to_partitioned():
    """Migrer les anciens parquets vers structure partitionnÃ©e"""
    print("ğŸš€ DÃ©marrage migration vers structure partitionnÃ©e Y/M/D")
    
    spark = create_spark_session()
    
    try:
        print("ğŸ“– Lecture des anciens parquets de crypto-data...")
        old_df = spark.read.parquet("s3a://crypto-data/")
        
        print(f"   ğŸ“Š {old_df.count()} records Ã  migrer")
        print("   ğŸ“‹ SchÃ©ma des donnÃ©es:")
        old_df.printSchema()
        
        # Ajouter les colonnes de partitioning si elles n'existent pas
        if "year" not in old_df.columns:
            print("ğŸ”§ Ajout des colonnes de partitioning Y/M/D...")
            
            # Utiliser timestamp ou ingestion_timestamp selon disponibilitÃ©
            timestamp_col = "timestamp" if "timestamp" in old_df.columns else "ingestion_timestamp"
            
            partitioned_df = old_df \
                .withColumn("timestamp_dt", to_timestamp(col(timestamp_col))) \
                .withColumn("year", year(col("timestamp_dt"))) \
                .withColumn("month", month(col("timestamp_dt"))) \
                .withColumn("day", dayofmonth(col("timestamp_dt")))
        else:
            print("âœ… Colonnes de partitioning dÃ©jÃ  prÃ©sentes")
            partitioned_df = old_df
        
        print("ğŸ’¾ Ã‰criture dans crypto-data-partitioned avec structure Y/M/D...")
        partitioned_df.write \
            .mode("append") \
            .partitionBy("year", "month", "day") \
            .option("compression", "snappy") \
            .parquet("s3a://crypto-data-partitioned/")
        
        print("âœ… Migration terminÃ©e avec succÃ¨s !")
        
        # Statistiques finales
        final_df = spark.read.parquet("s3a://crypto-data-partitioned/")
        print(f"ğŸ“ˆ DonnÃ©es finales: {final_df.count()} records")
        
        partitions = final_df.select("year", "month", "day").distinct().collect()
        print("ğŸ“ Partitions crÃ©Ã©es:")
        for p in partitions:
            print(f"   ğŸ“‚ year={p.year}/month={p.month}/day={p.day}")
            
    except Exception as e:
        print(f"âŒ Erreur migration: {e}")
    finally:
        spark.stop()

if __name__ == "__main__":
    migrate_to_partitioned()
