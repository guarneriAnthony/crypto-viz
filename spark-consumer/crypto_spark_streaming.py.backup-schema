"""
CryptoViz V3.0 - Spark Streaming Consumer avec PARTITIONING Y/M/D
Version optimisÃ©e avec partitioning S3 et compaction
"""

import os
import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import json

class CryptoSparkStreaming:
    def __init__(self):
        self.spark = self.create_spark_session()
        self.setup_minio_buckets()
        
    def create_spark_session(self):
        """CrÃ©er une session Spark optimisÃ©e pour partitioning"""
        return SparkSession.builder \
            .appName("CryptoViz-V3-Streaming-Partitioned") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.sql.adaptive.advisoryPartitionSizeInBytes", "128MB") \
            .config("spark.sql.streaming.forceDeleteTempCheckpointLocation", "true") \
            .config("spark.hadoop.fs.s3a.endpoint", os.getenv("MINIO_ENDPOINT", "http://minio:9000")) \
            .config("spark.hadoop.fs.s3a.access.key", os.getenv("MINIO_ACCESS_KEY", "cryptoviz")) \
            .config("spark.hadoop.fs.s3a.secret.key", os.getenv("MINIO_SECRET_KEY", "cryptoviz2024")) \
            .config("spark.hadoop.fs.s3a.path.style.access", "true") \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
            .config("spark.jars.packages", 
                   "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,"
                   "org.apache.hadoop:hadoop-aws:3.3.4,"
                   "com.amazonaws:aws-java-sdk-bundle:1.12.262") \
            .getOrCreate()

    def setup_minio_buckets(self):
        """Setup MinIO buckets avec nouvelle structure"""
        import boto3
        try:
            s3_client = boto3.client(
                's3',
                endpoint_url=os.getenv("MINIO_ENDPOINT", "http://minio:9000"),
                aws_access_key_id=os.getenv("MINIO_ACCESS_KEY", "cryptoviz"),
                aws_secret_access_key=os.getenv("MINIO_SECRET_KEY", "cryptoviz2024")
            )
            
            buckets = ['crypto-data-partitioned', 'crypto-data-archive', 'crypto-ml', 'crypto-catalog']
            for bucket in buckets:
                try:
                    s3_client.create_bucket(Bucket=bucket)
                    print(f"âœ… Bucket crÃ©Ã©: {bucket}")
                except:
                    pass  # Bucket existe dÃ©jÃ 
                    
        except Exception as e:
            print(f"âš ï¸ Erreur setup buckets: {e}")

    def process_crypto_stream(self):
        """Traiter les messages crypto avec PARTITIONING Y/M/D"""
        print("ğŸ“¡ DÃ©marrage streaming avec partitioning Y/M/D")
        
        # SchÃ©ma des donnÃ©es crypto
        crypto_schema = StructType([
            StructField("name", StringType(), True),
            StructField("symbol", StringType(), True),
            StructField("price", DoubleType(), True),
            StructField("market_cap", DoubleType(), True),
            StructField("volume_24h", DoubleType(), True),
            StructField("change_1h", DoubleType(), True),
            StructField("change_24h", DoubleType(), True),
            StructField("change_7d", DoubleType(), True),
            StructField("source", StringType(), True),
            StructField("timestamp", StringType(), True),
            StructField("ingestion_timestamp", StringType(), True)
        ])

        # Lecture stream Redpanda
        df = self.spark \
            .readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", os.getenv("REDPANDA_BROKERS", "redpanda:9092")) \
            .option("subscribe", "crypto-raw-data") \
            .option("startingOffsets", "latest") \
            .option("failOnDataLoss", "false") \
            .load()

        # Parse JSON et transformation avec colonnes partitioning
        parsed_df = df.select(
            from_json(col("value").cast("string"), crypto_schema).alias("data"),
            col("offset"),
            col("partition"),
            col("timestamp").alias("kafka_timestamp")
        ).select("data.*", "kafka_timestamp", "offset", "partition")

        # Enrichissement avec colonnes de partitioning optimisÃ©es
        enriched_df = parsed_df \
            .withColumn("processing_time", current_timestamp()) \
            .withColumn("timestamp_dt", to_timestamp(col("timestamp"), "yyyy-MM-dd'T'HH:mm:ss.SSSSSS'Z'")) \
            .withColumn("year", year(col("timestamp_dt"))) \
            .withColumn("month", month(col("timestamp_dt"))) \
            .withColumn("day", dayofmonth(col("timestamp_dt"))) \
            .withColumn("hour", hour(col("timestamp_dt"))) \
            .withColumn("date_partition", date_format(col("timestamp_dt"), "yyyy-MM-dd"))

        print("ğŸ—‚ï¸ Structure partitioning: year/month/day")
        print("â° DÃ©clenchement: toutes les 2 minutes (120s)")

        # Ã‰criture streaming avec PARTITIONING
        query = enriched_df.writeStream \
            .format("parquet") \
            .outputMode("append") \
            .option("checkpointLocation", "/tmp/spark-checkpoint-partitioned") \
            .trigger(processingTime="120 seconds") \
            .foreachBatch(self.process_partitioned_batch) \
            .start()

        return query

    def process_partitioned_batch(self, batch_df, batch_id):
        """Process batch avec partitioning Y/M/D et compaction"""
        count = batch_df.count()
        print(f"ğŸ“¦ Processing batch {batch_id} - {count} records")
        
        if count > 0:
            # Stats des cryptos traitÃ©s
            crypto_counts = batch_df.groupBy("symbol", "source").count().collect()
            for row in crypto_counts:
                print(f"   ğŸ’° {row['symbol']} ({row['source']}): {row['count']} records")
            
            try:
                # Ã‰CRITURE PARTITIONNÃ‰E Y/M/D
                batch_df.write \
                    .mode("append") \
                    .option("compression", "snappy") \
                    .partitionBy("year", "month", "day") \
                    .option("maxRecordsPerFile", 10000) \
                    .parquet("s3a://crypto-data-partitioned/crypto_prices/")
                    
                print(f"âœ… Batch {batch_id} Ã©crit avec partitioning Y/M/D")
                
                # Logs de la structure crÃ©Ã©e
                current_partitions = batch_df.select("year", "month", "day").distinct().collect()
                for partition in current_partitions:
                    year, month, day = partition['year'], partition['month'], partition['day']
                    print(f"   ğŸ“ Partition crÃ©Ã©e: year={year}/month={month:02d}/day={day:02d}")
                
            except Exception as e:
                print(f"âŒ Erreur Ã©criture batch {batch_id}: {e}")

    def compact_daily_partitions(self):
        """Compaction des partitions quotidiennes (optionnel)"""
        print("ğŸ—œï¸ Compaction des partitions en cours...")
        
        # Lire les donnÃ©es de la journÃ©e actuelle et rÃ©Ã©crire avec moins de fichiers
        from datetime import datetime
        today = datetime.now()
        
        try:
            daily_path = f"s3a://crypto-data-partitioned/crypto_prices/year={today.year}/month={today.month:02d}/day={today.day:02d}/"
            
            daily_df = self.spark.read.parquet(daily_path)
            
            # RÃ©Ã©crire avec moins de fichiers
            daily_df.coalesce(1) \
                .write \
                .mode("overwrite") \
                .option("compression", "snappy") \
                .parquet(daily_path)
                
            print(f"âœ… Compaction terminÃ©e pour {today.strftime('%Y-%m-%d')}")
            
        except Exception as e:
            print(f"âš ï¸ Compaction Ã©chouÃ©e: {e}")

    def run(self):
        """DÃ©marre le pipeline complet avec partitioning"""
        print("ğŸš€ CryptoViz V3 - Streaming avec Partitioning Y/M/D")
        
        # Attendre que les services soient prÃªts
        print("â³ Attente services (Redpanda, MinIO)...")
        time.sleep(30)
        
        try:
            # DÃ©marrer le streaming partitionnÃ©
            query = self.process_crypto_stream()
            
            print("ğŸ—‚ï¸ Pipeline partitionnÃ© en cours. Structure: year/month/day")
            print("ğŸ“Š Nouveau bucket: crypto-data-partitioned")
            print("ğŸ”„ Ctrl+C pour arrÃªter.")
            
            # Attendre indÃ©finiment (streaming continu)
            query.awaitTermination()
            
        except KeyboardInterrupt:
            print("ğŸ›‘ ArrÃªt du streaming demandÃ©")
            if 'query' in locals():
                query.stop()
        except Exception as e:
            print(f"âŒ Erreur pipeline: {e}")
        finally:
            print("ğŸ”’ Fermeture Spark Session")
            self.spark.stop()

def main():
    pipeline = CryptoSparkStreaming()
    pipeline.run()

if __name__ == "__main__":
    main()
