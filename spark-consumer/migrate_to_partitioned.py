"""
Script de migration CryptoViz V3.0
Migration des donn√©es existantes vers structure partitionn√©e Y/M/D
"""

import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from datetime import datetime

class CryptoDataMigration:
    def __init__(self):
        self.spark = self.create_spark_session()
        
    def create_spark_session(self):
        """Cr√©er session Spark pour migration"""
        return SparkSession.builder \
            .appName("CryptoViz-Migration-Partitioned") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.hadoop.fs.s3a.endpoint", os.getenv("MINIO_ENDPOINT", "http://minio:9000")) \
            .config("spark.hadoop.fs.s3a.access.key", os.getenv("MINIO_ACCESS_KEY", "cryptoviz")) \
            .config("spark.hadoop.fs.s3a.secret.key", os.getenv("MINIO_SECRET_KEY", "cryptoviz2024")) \
            .config("spark.hadoop.fs.s3a.path.style.access", "true") \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
            .config("spark.jars.packages", 
                   "org.apache.hadoop:hadoop-aws:3.3.4,"
                   "com.amazonaws:aws-java-sdk-bundle:1.12.262") \
            .getOrCreate()

    def migrate_existing_data(self):
        """Migrer les donn√©es existantes vers structure partitionn√©e"""
        print("üîÑ Migration des donn√©es vers structure partitionn√©e Y/M/D")
        
        try:
            # Lire toutes les donn√©es existantes
            print("üìñ Lecture des donn√©es existantes...")
            old_df = self.spark.read.parquet("s3a://crypto-data/")
            
            total_records = old_df.count()
            print(f"üìä Total enregistrements √† migrer: {total_records}")
            
            if total_records == 0:
                print("‚ö†Ô∏è Aucune donn√©e √† migrer")
                return
            
            # V√©rifier les colonnes disponibles
            columns = old_df.columns
            print(f"üìã Colonnes disponibles: {columns}")
            
            # Nettoyer et enrichir les donn√©es
            if 'timestamp_dt' in columns:
                # Les colonnes de partitioning existent d√©j√†
                clean_df = old_df
            else:
                # Cr√©er les colonnes de partitioning
                clean_df = old_df.withColumn("timestamp_dt", to_timestamp(col("timestamp")))
            
            # S'assurer que les colonnes year/month/day existent
            if 'year' not in columns:
                clean_df = clean_df \
                    .withColumn("year", year(col("timestamp_dt"))) \
                    .withColumn("month", month(col("timestamp_dt"))) \
                    .withColumn("day", dayofmonth(col("timestamp_dt")))
            
            # Filtrer les donn√©es valides
            valid_df = clean_df.filter(col("timestamp_dt").isNotNull())
            valid_count = valid_df.count()
            print(f"‚úÖ Enregistrements valides: {valid_count}")
            
            # Afficher les partitions qui seront cr√©√©es
            partitions_preview = valid_df.select("year", "month", "day").distinct().orderBy("year", "month", "day").collect()
            print("üìÅ Partitions qui seront cr√©√©es:")
            for partition in partitions_preview:
                year, month, day = partition['year'], partition['month'], partition['day']
                print(f"   year={year}/month={month:02d}/day={day:02d}")
            
            # √âcriture partitionn√©e
            print("üíæ √âcriture des donn√©es partitionn√©es...")
            
            valid_df.write \
                .mode("overwrite") \
                .option("compression", "snappy") \
                .partitionBy("year", "month", "day") \
                .option("maxRecordsPerFile", 10000) \
                .parquet("s3a://crypto-data-partitioned/crypto_prices/")
            
            print("‚úÖ Migration termin√©e avec succ√®s !")
            
            # Statistiques de migration
            self.migration_stats()
            
        except Exception as e:
            print(f"‚ùå Erreur pendant la migration: {e}")
            import traceback
            traceback.print_exc()

    def migration_stats(self):
        """Afficher les statistiques de migration"""
        print("\nüìä STATISTIQUES DE MIGRATION")
        print("="*50)
        
        try:
            # Lire les nouvelles donn√©es partitionn√©es
            partitioned_df = self.spark.read.parquet("s3a://crypto-data-partitioned/crypto_prices/")
            
            total = partitioned_df.count()
            print(f"üìà Total enregistrements migr√©s: {total}")
            
            # Stats par partition
            partition_stats = partitioned_df.groupBy("year", "month", "day").count().orderBy("year", "month", "day").collect()
            
            print("\nüìÅ Enregistrements par partition:")
            for stat in partition_stats:
                year, month, day = stat['year'], stat['month'], stat['day']
                count = stat['count']
                print(f"   year={year}/month={month:02d}/day={day:02d}: {count} records")
            
            # Stats par crypto
            print(f"\nüí∞ Cryptos disponibles:")
            crypto_stats = partitioned_df.groupBy("symbol").count().orderBy(desc("count")).limit(10).collect()
            for stat in crypto_stats:
                print(f"   {stat['symbol']}: {stat['count']} records")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur stats: {e}")

    def create_migration_buckets(self):
        """Cr√©er les buckets n√©cessaires pour la migration"""
        import boto3
        
        try:
            s3_client = boto3.client(
                's3',
                endpoint_url=os.getenv("MINIO_ENDPOINT", "http://minio:9000"),
                aws_access_key_id=os.getenv("MINIO_ACCESS_KEY", "cryptoviz"),
                aws_secret_access_key=os.getenv("MINIO_SECRET_KEY", "cryptoviz2024")
            )
            
            buckets = ['crypto-data-partitioned', 'crypto-data-archive']
            for bucket in buckets:
                try:
                    s3_client.create_bucket(Bucket=bucket)
                    print(f"‚úÖ Bucket cr√©√©: {bucket}")
                except:
                    print(f"‚ÑπÔ∏è Bucket existe d√©j√†: {bucket}")
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur cr√©ation buckets: {e}")

    def run_migration(self):
        """Ex√©cuter la migration compl√®te"""
        print("üöÄ D√âMARRAGE MIGRATION CRYPTOVIZ V3.0")
        print("="*50)
        
        try:
            # Cr√©er les buckets
            self.create_migration_buckets()
            
            # Migrer les donn√©es
            self.migrate_existing_data()
            
            print("\nüéâ MIGRATION R√âUSSIE !")
            print("üìã Prochaines √©tapes:")
            print("   1. Red√©marrer Spark streaming avec nouveau code")
            print("   2. Adapter le dashboard pour nouvelle structure")
            print("   3. (Optionnel) Archiver ancien bucket crypto-data")
            
        except Exception as e:
            print(f"üí• Erreur migration: {e}")
        finally:
            print("üîí Fermeture session Spark")
            self.spark.stop()

def main():
    migrator = CryptoDataMigration()
    migrator.run_migration()

if __name__ == "__main__":
    main()
